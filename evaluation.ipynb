{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb58794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common import plot_util as pu\n",
    "import numpy as np\n",
    "import gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e4f36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de61a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96360d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['Reacher-v2',\n",
    "        'InvertedPendulum-v2',\n",
    "        'Walker2d-v2',\n",
    "        'Humanoid-v2',\n",
    "        'HalfCheetah-v2',\n",
    "        'InvertedDoublePendulum-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1306b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {}\n",
    "thresholds['Reacher-v2'] = -7\n",
    "thresholds['InvertedPendulum-v2'] = 950\n",
    "thresholds['Humanoid-v2'] = 2500\n",
    "thresholds['Walker2d-v2'] = 3000\n",
    "thresholds['HalfCheetah-v2'] = 4700\n",
    "thresholds['InvertedDoublePendulum-v2'] = 9100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d7ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = '/home/danielzgsilva/Documents/robotics_project/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7042349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = ['trpo_mpi', \n",
    "        'ddpg',\n",
    "        'acktr',\n",
    "        'ppo2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a062c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top_n_rewards(n, rewards):\n",
    "    return np.mean(rewards[np.argsort(rewards)][-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2843a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_window_past_threshold(window, threshold, rewards):\n",
    "    flag = False\n",
    "    for i in range(len(rewards) - window):\n",
    "        window_mean = np.mean(rewards[i:i+window])\n",
    "        if window_mean >= threshold:\n",
    "            return i\n",
    "\n",
    "    if not flag:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75483ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Env: Reacher-v2  Alg: trpo_mpi --------\n",
      "Average reward of top 20 episodes: -0.8435946000000001\n",
      "Episodes to reach -7 reward: 2795\n",
      "\n",
      "------- Env: Reacher-v2  Alg: ddpg --------\n",
      "Average reward of top 20 episodes: -25.52634885\n",
      "Episodes to reach -7 reward: -1\n",
      "\n",
      "------- Env: Reacher-v2  Alg: acktr --------\n",
      "Average reward of top 20 episodes: -35.665557050000004\n",
      "Episodes to reach -7 reward: -1\n",
      "\n",
      "------- Env: Reacher-v2  Alg: ppo2 --------\n",
      "Average reward of top 20 episodes: -0.97445795\n",
      "Episodes to reach -7 reward: 2760\n",
      "\n",
      "------- Env: InvertedPendulum-v2  Alg: trpo_mpi --------\n",
      "Average reward of top 20 episodes: 1000.0\n",
      "Episodes to reach 950 reward: 493\n",
      "\n",
      "------- Env: InvertedPendulum-v2  Alg: ddpg --------\n",
      "Average reward of top 20 episodes: 1000.0\n",
      "Episodes to reach 950 reward: 247\n",
      "\n",
      "------- Env: InvertedPendulum-v2  Alg: acktr --------\n",
      "Average reward of top 20 episodes: 430.9\n",
      "Episodes to reach 950 reward: -1\n",
      "\n",
      "------- Env: InvertedPendulum-v2  Alg: ppo2 --------\n",
      "Average reward of top 20 episodes: 1000.0\n",
      "Episodes to reach 950 reward: 874\n",
      "\n",
      "------- Env: Walker2d-v2  Alg: trpo_mpi --------\n",
      "Average reward of top 20 episodes: 4193.645878949999\n",
      "Episodes to reach 3000 reward: 2001\n",
      "\n",
      "------- Env: Walker2d-v2  Alg: ddpg --------\n",
      "Average reward of top 20 episodes: 1243.5618468\n",
      "Episodes to reach 3000 reward: -1\n",
      "\n",
      "------- Env: Walker2d-v2  Alg: acktr --------\n",
      "Average reward of top 20 episodes: 802.7179512\n",
      "Episodes to reach 3000 reward: -1\n",
      "\n",
      "------- Env: Walker2d-v2  Alg: ppo2 --------\n",
      "Average reward of top 20 episodes: 4609.0401556\n",
      "Episodes to reach 3000 reward: 2621\n",
      "\n",
      "------- Env: Humanoid-v2  Alg: trpo_mpi --------\n",
      "Average reward of top 20 episodes: 1018.96064555\n",
      "Episodes to reach 2500 reward: -1\n",
      "\n",
      "------- Env: Humanoid-v2  Alg: ddpg --------\n",
      "Average reward of top 20 episodes: 1141.70911135\n",
      "Episodes to reach 2500 reward: -1\n",
      "\n",
      "------- Env: Humanoid-v2  Alg: acktr --------\n",
      "Average reward of top 20 episodes: 936.5450119500001\n",
      "Episodes to reach 2500 reward: -1\n",
      "\n",
      "------- Env: Humanoid-v2  Alg: ppo2 --------\n",
      "Average reward of top 20 episodes: 1182.5475252999997\n",
      "Episodes to reach 2500 reward: -1\n",
      "\n",
      "------- Env: HalfCheetah-v2  Alg: trpo_mpi --------\n",
      "Average reward of top 20 episodes: 1026.7489896000002\n",
      "Episodes to reach 4700 reward: -1\n",
      "\n",
      "------- Env: HalfCheetah-v2  Alg: ddpg --------\n",
      "Average reward of top 20 episodes: -237.89283665\n",
      "Episodes to reach 4700 reward: -1\n",
      "\n",
      "------- Env: HalfCheetah-v2  Alg: acktr --------\n",
      "Average reward of top 20 episodes: -6.759316550000001\n",
      "Episodes to reach 4700 reward: -1\n",
      "\n",
      "------- Env: HalfCheetah-v2  Alg: ppo2 --------\n",
      "Average reward of top 20 episodes: 1491.1509793500002\n",
      "Episodes to reach 4700 reward: -1\n",
      "\n",
      "------- Env: InvertedDoublePendulum-v2  Alg: trpo_mpi --------\n",
      "Average reward of top 20 episodes: 9347.0120271\n",
      "Episodes to reach 9100 reward: 2584\n",
      "\n",
      "------- Env: InvertedDoublePendulum-v2  Alg: ddpg --------\n",
      "Average reward of top 20 episodes: 9358.629520049999\n",
      "Episodes to reach 9100 reward: 1578\n",
      "\n",
      "------- Env: InvertedDoublePendulum-v2  Alg: acktr --------\n",
      "Average reward of top 20 episodes: 2469.40335045\n",
      "Episodes to reach 9100 reward: -1\n",
      "\n",
      "------- Env: InvertedDoublePendulum-v2  Alg: ppo2 --------\n",
      "Average reward of top 20 episodes: 9327.167597\n",
      "Episodes to reach 9100 reward: 3874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n = 20\n",
    "window_size = 1\n",
    "\n",
    "for env in envs:\n",
    "    for alg in algs:\n",
    "        exp_path = os.path.join(logs_dir, env, alg)\n",
    "        print('------- Env: {}  Alg: {} --------'.format(env, alg))\n",
    "        \n",
    "        results = results = pu.load_results(exp_path)[0]\n",
    "        rewards = np.array(results.monitor.r)\n",
    "        \n",
    "        top_rewards = calc_top_n_rewards(top_n, rewards)\n",
    "        episode_to_thresh = episode_window_past_threshold(window_size, \n",
    "                                                         thresholds[env],\n",
    "                                                         rewards)\n",
    "        \n",
    "        print('Average reward of top {} episodes: {}'.format(top_n, top_rewards))\n",
    "        print('Episodes to reach {} reward: {}'.format(thresholds[env], episode_to_thresh))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
